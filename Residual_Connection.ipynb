{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This project demonstrates the effect of residual (shortcut) connections on gradient flow in deep fully connected neural networks using PyTorch. It compares the magnitude of gradients in each layer with and without residual connections after a single backward pass."
      ],
      "metadata": {
        "id": "siT-pbXQ_ZaE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pjvWIsVL_Wbe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, shortcut_connections = False):\n",
        "        super().__init__()\n",
        "        self.shortcut_connections = shortcut_connections  # store in self\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.Linear(10, 10, bias=False),\n",
        "            nn.Linear(10, 10, bias=False),\n",
        "            nn.Linear(10, 10, bias=False),\n",
        "            nn.Linear(10, 10, bias=False),\n",
        "            nn.Linear(10, 1, bias=False)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.shortcut_connections:\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                if i < len(self.layers) - 1:\n",
        "                    residual = x\n",
        "                    x = F.relu(layer(x))\n",
        "                    x = x + residual\n",
        "                else:\n",
        "                    x = layer(x)\n",
        "            return x\n",
        "        # if no shortcut_connections is false\n",
        "        else:\n",
        "            # Apply ReLU only on all layers except the last one\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                if i < len(self.layers) - 1:\n",
        "                    x = F.relu(layer(x))\n",
        "                else:\n",
        "                    x = layer(x)\n",
        "            return x\n",
        "\n",
        "\n",
        "def output(x):\n",
        "    model = NeuralNetwork(shortcut_connections = True)\n",
        "    out = model(x)\n",
        "    targets = torch.tensor([[0.]])\n",
        "    criterion = nn.MSELoss()\n",
        "    loss = criterion(out, targets)\n",
        "    loss.backward()\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"weight\" in name:\n",
        "            print(f\"{name} has an average gradient of: {param.grad.abs().mean().item()}\")"
      ],
      "metadata": {
        "id": "YXigWPlCAd1b"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZDyxv2GeJn98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "sample_input = torch.randn(1, 10)\n",
        "output(sample_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLpOizj9GSk4",
        "outputId": "c112c51e-dcee-4fe9-c5a5-01b4ae658fc4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.weight has an average gradient of: 0.010994031094014645\n",
            "layers.1.weight has an average gradient of: 0.006011884659528732\n",
            "layers.2.weight has an average gradient of: 0.011236266233026981\n",
            "layers.3.weight has an average gradient of: 0.008820828050374985\n",
            "layers.4.weight has an average gradient of: 0.10310874879360199\n"
          ]
        }
      ]
    }
  ]
}